{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b177f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flask\n",
    "from flask import Flask, render_template, url_for, request\n",
    "from flask_ngrok import run_with_ngrok\n",
    "\n",
    "# imports\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2220467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "\n",
    "# Tokenization des Tags\n",
    "def delete_element(liste):\n",
    "    element_to_delete = '>'\n",
    "    try:\n",
    "        while True:\n",
    "            liste.remove(element_to_delete)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return liste\n",
    "\n",
    "def Tokenization_Tags(data):\n",
    "    x = []\n",
    "    y = []\n",
    "    for row in data['Tags']:\n",
    "        x.append(row.split('<'))\n",
    "    for i in range(len(x)):\n",
    "        x[i].remove('')\n",
    "    data['Tags']=x\n",
    "    return data\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def element_to_delete_2(data):\n",
    "    element_to_remove = '>'\n",
    "    x = data['Tags']\n",
    "    y = []\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(x[i])):\n",
    "            x[i][j]=x[i][j].replace('>','')\n",
    "    data['Tags'] = x\n",
    "    return data\n",
    "\n",
    "\n",
    "# reduction du numbre de tags Ã  10\n",
    "def Tags_reducing(data):\n",
    "    tags = data['Tags']\n",
    "    unique_tag_word = []\n",
    "    for row in tags:\n",
    "        for word in row:\n",
    "            unique_tag_word.append(word)\n",
    "    test = pd.DataFrame()\n",
    "    test['unique_tag_word'] = unique_tag_word\n",
    "    data_count = test.unique_tag_word.value_counts()\n",
    "    tags_df = pd.DataFrame(data = np.zeros((len(data),len(data_count[0:10]))),columns=test.unique_tag_word.value_counts()[0:10].sort_values(ascending = False).index)\n",
    "    a = 0\n",
    "    while a <4 :\n",
    "        a = a+1\n",
    "        for i in range(len(data['Tags'])):\n",
    "            for word in data['Tags'][i]:\n",
    "                if word not in tags_df.columns:\n",
    "                      data['Tags'][i].remove(word)\n",
    "    empty_list = []\n",
    "    for i in range(len(data)):\n",
    "        if data['Tags'][i] == empty_list:\n",
    "              data = data.drop(i)\n",
    "    data = data.reset_index()\n",
    "    for i in range(len(data)):\n",
    "        for word in data['Tags'][i]:\n",
    "            tags_df[word][i]=1\n",
    "    data = data.drop(columns = 'index')\n",
    "    data = data.reset_index()\n",
    "    tags_df = tags_df.reset_index()\n",
    "    data = data.merge(tags_df,on='index',how='left')\n",
    "    return data\n",
    "\n",
    "def iml_p5_preprocessing_Tags(data):\n",
    "    data = Tokenization_Tags(data)\n",
    "    data = element_to_delete_2(data)\n",
    "    data = Tags_reducing(data)\n",
    "    #data = data.drop(columns=['Tags'])\n",
    "    return data\n",
    "\n",
    "# tokenization Body\n",
    "def Tokenization_texte(texte):\n",
    "    x = []\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    for row in texte:\n",
    "        x.append(tokenizer.tokenize(row))\n",
    "    return x\n",
    "\n",
    "# stemmatization Body\n",
    "def text_stematization(texte):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stem_word = []\n",
    "    stem_sentence = []\n",
    "    for i in range(len(texte)):\n",
    "        for word in texte[i]:\n",
    "            stem_word.append(stemmer.stem(word))\n",
    "        stem_sentence.append(stem_word)\n",
    "        stem_word = [] \n",
    "    return stem_sentence\n",
    "\n",
    "\n",
    "# lemmatization Body\n",
    "def text_lemmatization(texte):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_word = []\n",
    "    lem_sentence = []\n",
    "    for i in range(len(texte)):\n",
    "        for word in texte[i]:\n",
    "            lem_word.append(lemmatizer.lemmatize(word))\n",
    "        lem_sentence.append(lem_word)\n",
    "        lem_word = [] \n",
    "    return lem_sentence\n",
    "    \n",
    "# removing stopwords Body\n",
    "def delete_stopwords(texte):\n",
    "    cleaned_sentence = []\n",
    "    cleaned_texte = []\n",
    "    for sentence in texte:\n",
    "        for word in sentence:\n",
    "            if word not in stopwords.words('english'):\n",
    "                cleaned_sentence.append(word)\n",
    "        cleaned_texte.append(cleaned_sentence)\n",
    "        cleaned_sentence = []\n",
    "    return cleaned_texte\n",
    "\n",
    "# removing frequent_words Body\n",
    "def delete_frequent_words(texte,all_words_df):\n",
    "    \n",
    "    frequent_words = list(all_words_Body_df.text_words.value_counts().sort_values(ascending=False)[0:50].index)\n",
    "    to_keep = ['string', 'http', 'java', 'android', 'list']\n",
    "    for word in to_keep:\n",
    "        frequent_words.remove(word)\n",
    "    cleaned_sentence = []\n",
    "    cleaned_texte = []\n",
    "    for sentence in texte:\n",
    "        for word in sentence:\n",
    "            if word not in frequent_words:\n",
    "                cleaned_sentence.append(word)\n",
    "        cleaned_texte.append(cleaned_sentence)\n",
    "        cleaned_sentence = []\n",
    "    return cleaned_texte\n",
    "\n",
    "\n",
    "def text_reconstruction(texte):\n",
    "    return \" \".join([word for word in texte])  \n",
    "\n",
    "# text_cleaning Body\n",
    "def text_cleaning(texte,all_words_df):\n",
    "    cleaned_texte = Tokenization_texte(texte)\n",
    "    cleaned_texte = text_stematization(cleaned_texte)\n",
    "    cleaned_texte = text_lemmatization(cleaned_texte)\n",
    "    cleaned_texte = delete_stopwords(cleaned_texte)\n",
    "    cleaned_texte = delete_frequent_words(cleaned_texte,all_words_df)\n",
    "    reconstructed_text = []\n",
    "    for i in range(len(cleaned_texte)):\n",
    "        reconstructed_sentence = text_reconstruction(cleaned_texte[i])\n",
    "        reconstructed_text.append(reconstructed_sentence)\n",
    "        reconstructed_sentence = []\n",
    "    return reconstructed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3546bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_categorie(predictions):\n",
    "        cat=['java', 'c#', 'javascript', 'python', 'jquery', 'html', 'php',\n",
    "       'android', '.net', 'c++', 'sql', 'css', 'mysql', 'string', 'sql-server',\n",
    "       'c', 'git', 'arrays', 'asp.net', 'linux', 'regex', 'iphone', 'bash',\n",
    "       'objective-c', 'ios', 'windows', 'ruby', 'eclipse', 'tsql', 'database']\n",
    "        predicted_cat = []\n",
    "        empty_list = []\n",
    "        for i in range((len(cat)-1)):\n",
    "            if predictions[i][0]==1:\n",
    "                predicted_cat.append(cat[i])\n",
    "        if predicted_cat == empty_list:\n",
    "            predicted_cat.append('unknown')\n",
    "        return predicted_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee6e536",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_words_Body_df = pd.read_csv('/home/abdifatah/Ingenieur_Machine_Learning/all_words_Body_df.csv',sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
