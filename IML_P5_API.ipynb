{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf09b8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flask\n",
    "from flask import Flask, render_template, url_for, request\n",
    "from flask_ngrok import run_with_ngrok\n",
    "\n",
    "# imports\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50157f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "\n",
    "# Tokenization des Tags\n",
    "def delete_element(liste):\n",
    "    element_to_delete = '>'\n",
    "    try:\n",
    "        while True:\n",
    "            liste.remove(element_to_delete)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return liste\n",
    "\n",
    "def Tokenization_Tags(data):\n",
    "    x = []\n",
    "    y = []\n",
    "    for row in data['Tags']:\n",
    "        x.append(row.split('<'))\n",
    "    for i in range(len(x)):\n",
    "        x[i].remove('')\n",
    "    data['Tags']=x\n",
    "    return data\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def element_to_delete_2(data):\n",
    "    element_to_remove = '>'\n",
    "    x = data['Tags']\n",
    "    y = []\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(x[i])):\n",
    "            x[i][j]=x[i][j].replace('>','')\n",
    "    data['Tags'] = x\n",
    "    return data\n",
    "\n",
    "\n",
    "# reduction du numbre de tags à 10\n",
    "def Tags_reducing(data):\n",
    "    tags = data['Tags']\n",
    "    unique_tag_word = []\n",
    "    for row in tags:\n",
    "        for word in row:\n",
    "            unique_tag_word.append(word)\n",
    "    test = pd.DataFrame()\n",
    "    test['unique_tag_word'] = unique_tag_word\n",
    "    data_count = test.unique_tag_word.value_counts()\n",
    "    tags_df = pd.DataFrame(data = np.zeros((len(data),len(data_count[0:10]))),columns=test.unique_tag_word.value_counts()[0:10].sort_values(ascending = False).index)\n",
    "    a = 0\n",
    "    while a <4 :\n",
    "        a = a+1\n",
    "        for i in range(len(data['Tags'])):\n",
    "            for word in data['Tags'][i]:\n",
    "                if word not in tags_df.columns:\n",
    "                      data['Tags'][i].remove(word)\n",
    "    empty_list = []\n",
    "    for i in range(len(data)):\n",
    "        if data['Tags'][i] == empty_list:\n",
    "              data = data.drop(i)\n",
    "    data = data.reset_index()\n",
    "    for i in range(len(data)):\n",
    "        for word in data['Tags'][i]:\n",
    "            tags_df[word][i]=1\n",
    "    data = data.drop(columns = 'index')\n",
    "    data = data.reset_index()\n",
    "    tags_df = tags_df.reset_index()\n",
    "    data = data.merge(tags_df,on='index',how='left')\n",
    "    return data\n",
    "\n",
    "def iml_p5_preprocessing_Tags(data):\n",
    "    data = Tokenization_Tags(data)\n",
    "    data = element_to_delete_2(data)\n",
    "    data = Tags_reducing(data)\n",
    "    #data = data.drop(columns=['Tags'])\n",
    "    return data\n",
    "\n",
    "# tokenization Body\n",
    "def Tokenization_texte(texte):\n",
    "    x = []\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    for row in texte:\n",
    "        x.append(tokenizer.tokenize(row))\n",
    "    return x\n",
    "\n",
    "# stemmatization Body\n",
    "def text_stematization(texte):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stem_word = []\n",
    "    stem_sentence = []\n",
    "    for i in range(len(texte)):\n",
    "        for word in texte[i]:\n",
    "            stem_word.append(stemmer.stem(word))\n",
    "        stem_sentence.append(stem_word)\n",
    "        stem_word = [] \n",
    "    return stem_sentence\n",
    "\n",
    "\n",
    "# lemmatization Body\n",
    "def text_lemmatization(texte):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_word = []\n",
    "    lem_sentence = []\n",
    "    for i in range(len(texte)):\n",
    "        for word in texte[i]:\n",
    "            lem_word.append(lemmatizer.lemmatize(word))\n",
    "        lem_sentence.append(lem_word)\n",
    "        lem_word = [] \n",
    "    return lem_sentence\n",
    "    \n",
    "# removing stopwords Body\n",
    "def delete_stopwords(texte):\n",
    "    cleaned_sentence = []\n",
    "    cleaned_texte = []\n",
    "    for sentence in texte:\n",
    "        for word in sentence:\n",
    "            if word not in stopwords.words('english'):\n",
    "                cleaned_sentence.append(word)\n",
    "        cleaned_texte.append(cleaned_sentence)\n",
    "        cleaned_sentence = []\n",
    "    return cleaned_texte\n",
    "\n",
    "# removing frequent_words Body\n",
    "def delete_frequent_words(texte,all_words_df):\n",
    "    \n",
    "    frequent_words = list(all_words_Body_df.text_words.value_counts().sort_values(ascending=False)[0:50].index)\n",
    "    to_keep = ['string', 'http', 'java', 'android', 'list']\n",
    "    for word in to_keep:\n",
    "        frequent_words.remove(word)\n",
    "    cleaned_sentence = []\n",
    "    cleaned_texte = []\n",
    "    for sentence in texte:\n",
    "        for word in sentence:\n",
    "            if word not in frequent_words:\n",
    "                cleaned_sentence.append(word)\n",
    "        cleaned_texte.append(cleaned_sentence)\n",
    "        cleaned_sentence = []\n",
    "    return cleaned_texte\n",
    "\n",
    "\n",
    "def text_reconstruction(texte):\n",
    "    return \" \".join([word for word in texte])  \n",
    "\n",
    "# text_cleaning Body\n",
    "def text_cleaning(texte,all_words_df):\n",
    "    cleaned_texte = Tokenization_texte(texte)\n",
    "    cleaned_texte = text_stematization(cleaned_texte)\n",
    "    cleaned_texte = text_lemmatization(cleaned_texte)\n",
    "    cleaned_texte = delete_stopwords(cleaned_texte)\n",
    "    cleaned_texte = delete_frequent_words(cleaned_texte,all_words_df)\n",
    "    reconstructed_text = []\n",
    "    for i in range(len(cleaned_texte)):\n",
    "        reconstructed_sentence = text_reconstruction(cleaned_texte[i])\n",
    "        reconstructed_text.append(reconstructed_sentence)\n",
    "        reconstructed_sentence = []\n",
    "    return reconstructed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a0a4758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_categorie(predictions):\n",
    "        cat=['java', 'c#', 'javascript', 'python', 'jquery', 'html', 'php',\n",
    "       'android', '.net', 'c++', 'sql', 'css', 'mysql', 'string', 'sql-server',\n",
    "       'c', 'git', 'arrays', 'asp.net', 'linux', 'regex', 'iphone', 'bash',\n",
    "       'objective-c', 'ios', 'windows', 'ruby', 'eclipse', 'tsql', 'database']\n",
    "        predicted_cat = []\n",
    "        empty_list = []\n",
    "        for i in range((len(cat)-1)):\n",
    "            if predictions[i][0]==1:\n",
    "                predicted_cat.append(cat[i])\n",
    "        if predicted_cat == empty_list:\n",
    "            predicted_cat.append('unknown')\n",
    "        return predicted_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "690a0c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(title, body):\n",
    "    text = []\n",
    "    text.append(title+''+body)\n",
    "    cleaned_text = text_cleaning(text,all_words_Body_df)\n",
    "    prediction = pd.DataFrame.sparse.from_spmatrix(model.predict(cleaned_text))\n",
    "    #prediction = predict_categorie(prediction)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00654939",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = joblib.load('cleaned_data.sav')\n",
    "all_words_Body_df = pd.read_csv('/home/abdifatah/Ingenieur_Machine_Learning/all_words_Body_df.csv',sep=',')\n",
    "model = joblib.load('prediction_pipeline.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09279a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_prediction('', \"Assigning TCP/IP Ports for python In-House Application Use <p>I've written a WCF Service hosted by a Windows Service and python it needs to listen on a known TCP/IP port.  From what range can I safely allocate a port for use within my organization?  That port will be embedded in the config files for the service and the clients that are consuming the service.</p>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0628fa8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "run_with_ngrok(app)\n",
    " \n",
    "@app.route('/', methods = ['GET','POST'])\n",
    "def main():\n",
    "    return \"\"\"<!DOCTYPE html>\n",
    "                <html>\n",
    "                    <head>\n",
    "                         <title>Catégoriser questions</title>\n",
    "                    </head>\n",
    "                    <body>\n",
    "                        <div align=\"center\" class=\"bg-info\">\n",
    "                            <h1>Prediction de Tags</h1>\n",
    "                        </div>\n",
    "                        <div class=\"big\" align=\"center\">\n",
    "                            <form action=\"predict\" method=\"POST\">\n",
    "                                <h3>Entrez un titre</h3>\n",
    "                                <textarea name=\"title\" rows=\"1\" cols=\"70\"></textarea>\n",
    "                                <br>\n",
    "                                <h3>Entez un contenu</h3>\n",
    "                                <textarea name=\"body\" rows=\"20\" cols=\"70\"></textarea>\n",
    "                                   <br><br><br>\n",
    "                               <input type=\"submit\" name=\"\" value=\"Predict\" class=\"btn btn-info\">\n",
    "                              </form>\n",
    "                         </div>\n",
    "                    </body>\n",
    "                </html>\n",
    "                \"\"\"\n",
    "\n",
    "@app.route('/predict', methods = ['POST'])\n",
    "def predict():\n",
    "    title = \"\"\n",
    "    body = \"\"\n",
    "    if request.method == 'POST':\n",
    "        title = str(request.form['title'])\n",
    "        body = str(request.form['body'])\n",
    "    tags = make_prediction(title, body)\n",
    "    return \"\"\"\n",
    "            <!DOCTYPE html>\n",
    "            <html>\n",
    "                <head>\n",
    "                     <title>Catégoriser questions</title>\n",
    "                </head>\n",
    "                <body>\n",
    "                    <div class=\"big\" align=\"center\">\n",
    "                                <h4>Titre saisi</h4>\n",
    "                                <textarea name=\"title\" rows=\"1\" cols=\"100\"disabled>\"\"\"+ title +\"\"\"</textarea>\n",
    "                                <br>\n",
    "                                <h4>Contenu saisi</h4>\n",
    "                                <textarea name=\"body\" rows=\"7\" cols=\"100\" disabled>\"\"\"+ body +\"\"\"</textarea>\n",
    "                                <br><br>\n",
    "                     </div>\n",
    "                     <div align=\"center\">\n",
    "                          <h2>Tags proposes :</h2>\n",
    "                          <textarea rows=\"5\" cols=\"50\" disabled>\"\"\"+ str(tags) +\"\"\"</textarea>\n",
    "                     </div>\n",
    "                     <br><br><br>\n",
    "                     <form action=\"/\" method=\"POST\" align=\"center\">\n",
    "                         <input type=\"submit\" name=\"\" value=\"Try again\" class=\"btn btn-info\">\n",
    "                     </form>\n",
    "                </body>\n",
    "            </html>\n",
    "        \"\"\"\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f4fb1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
